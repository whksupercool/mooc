{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 0.18085872, -0.14481524,  0.07108589,  0.15595298, -0.23238587,\n",
       "         -0.23406386, -0.21760766, -0.0375983 , -0.20175496,  0.23114355,\n",
       "         -0.11212598,  0.06660162,  0.13310091, -0.13525748, -0.1979692 ,\n",
       "          0.23103131,  0.01167171,  0.07423262,  0.21973999, -0.0080478 ,\n",
       "         -0.18773144, -0.12562071, -0.09352565,  0.12074257,  0.16998641,\n",
       "         -0.0947555 ,  0.12088688, -0.10150141, -0.01482262,  0.19589381,\n",
       "          0.01732112, -0.03531927, -0.23605229,  0.13926505, -0.15277608,\n",
       "          0.06522809, -0.18537559, -0.08938642, -0.05762003, -0.05860168,\n",
       "         -0.10969631, -0.07219073,  0.2113512 ,  0.08014159, -0.22397691,\n",
       "         -0.21620905, -0.21404532,  0.03883915, -0.12380588,  0.06324239,\n",
       "          0.06444888, -0.00528495, -0.19566634, -0.17228618, -0.11626617,\n",
       "          0.12305368,  0.18170758, -0.05937006, -0.04210889, -0.0474399 ,\n",
       "         -0.17786357,  0.12798898, -0.23587292, -0.2201182 , -0.03020792,\n",
       "          0.04420562, -0.13440675,  0.00399527, -0.1397247 , -0.14438689,\n",
       "         -0.21783209,  0.03593604,  0.13873701,  0.09408496,  0.19262873,\n",
       "         -0.12020313, -0.20916198,  0.01528631,  0.12517776, -0.0376627 ,\n",
       "          0.1289467 , -0.01454096,  0.18745236,  0.21447943,  0.08839144,\n",
       "         -0.03344238,  0.16577788, -0.07989408,  0.13261865,  0.05598246,\n",
       "          0.11867811, -0.17373301,  0.03598203,  0.10934405, -0.01689072,\n",
       "         -0.08933342,  0.02479233, -0.16815625, -0.11337748,  0.19084378],\n",
       "        [-0.180903  ,  0.12576793, -0.10530528,  0.02774976,  0.14897983,\n",
       "         -0.23185128,  0.22544836, -0.081609  , -0.18538243, -0.13713881,\n",
       "         -0.21815348,  0.09669541,  0.16622214, -0.0094333 ,  0.04503565,\n",
       "         -0.17626674,  0.20535983, -0.23049524,  0.13260461, -0.215121  ,\n",
       "         -0.19903669, -0.20232984,  0.19900067,  0.1223871 , -0.15052804,\n",
       "         -0.03226615,  0.16029902, -0.03706986, -0.22276774,  0.20808785,\n",
       "          0.10978834, -0.16541448, -0.22867802,  0.16701771,  0.04542987,\n",
       "          0.13590957, -0.20192952,  0.01009345,  0.17791624,  0.20513742,\n",
       "         -0.19332838,  0.18335758,  0.1877098 ,  0.12937842,  0.04081784,\n",
       "         -0.13664092,  0.09527545,  0.09667946,  0.17293175,  0.17815368,\n",
       "          0.1837136 ,  0.00101921,  0.1207564 ,  0.03520869,  0.22315939,\n",
       "         -0.09779066, -0.17866376,  0.05223729, -0.03192551,  0.076474  ,\n",
       "          0.06161438, -0.19776921,  0.0898553 ,  0.11974604, -0.08681507,\n",
       "         -0.22072284, -0.14028704,  0.03573407, -0.12244438, -0.14165339,\n",
       "         -0.15382987, -0.15114465, -0.04506865,  0.14904453, -0.01675302,\n",
       "          0.17311971,  0.08910777,  0.18883403,  0.08090337,  0.13906522,\n",
       "         -0.00449601,  0.2070729 ,  0.2268254 ,  0.03384064,  0.21671687,\n",
       "         -0.15312749,  0.23456956,  0.10700135,  0.12596329, -0.02210152,\n",
       "         -0.20344166, -0.1081098 ,  0.11132677,  0.07183932, -0.00367656,\n",
       "         -0.01244527, -0.13098927, -0.06588338,  0.20184632, -0.16616127],\n",
       "        [-0.06706986,  0.19656534,  0.22473685, -0.04295997, -0.1852107 ,\n",
       "          0.09004413, -0.21060112, -0.04754791, -0.01248801, -0.15092039,\n",
       "         -0.1017964 , -0.09039599,  0.15734874, -0.07085089, -0.21152367,\n",
       "          0.08544125, -0.13887453, -0.03691678,  0.12649237, -0.19826083,\n",
       "         -0.20239772, -0.21358378,  0.23722418, -0.12688476, -0.12357689,\n",
       "          0.04445733,  0.14510684,  0.07425548,  0.01008165,  0.2225246 ,\n",
       "         -0.21700752,  0.209462  , -0.10091467, -0.18094781,  0.00484742,\n",
       "          0.18873556,  0.14917977, -0.15058738,  0.06198861,  0.22439267,\n",
       "          0.23183356, -0.14548549, -0.1017732 , -0.16706997,  0.02571242,\n",
       "         -0.03159192,  0.16403861, -0.22558445,  0.03953041, -0.10159972,\n",
       "         -0.0318822 , -0.00396295, -0.07311225, -0.01671347,  0.16376938,\n",
       "         -0.13916491, -0.18490973, -0.16101868, -0.18013884,  0.05458511,\n",
       "          0.12713604, -0.06072022,  0.22028188, -0.20857924, -0.06742294,\n",
       "         -0.1563586 , -0.02629597, -0.11754994,  0.18102174,  0.0537668 ,\n",
       "         -0.04144265,  0.04806529, -0.14519379,  0.08873208,  0.00254554,\n",
       "         -0.06101437, -0.01332688, -0.20873386, -0.0882387 , -0.02771829,\n",
       "         -0.184174  ,  0.02293999,  0.11730565,  0.2229215 ,  0.16491209,\n",
       "         -0.148821  , -0.17750064, -0.09383859, -0.13813749,  0.09636568,\n",
       "         -0.06951554, -0.04129548, -0.08776343, -0.21490312, -0.15075973,\n",
       "          0.14765926, -0.02140905,  0.18920381, -0.2046172 , -0.17634386],\n",
       "        [-0.14361542,  0.11124469, -0.18759796,  0.14287321, -0.02269602,\n",
       "          0.14735766, -0.06155095,  0.16386814, -0.00778604,  0.08349313,\n",
       "          0.09263997, -0.03845462,  0.18499573,  0.11237977, -0.14497118,\n",
       "         -0.16651605, -0.15391862,  0.10765944, -0.21192843,  0.02421065,\n",
       "         -0.2049172 ,  0.11332847,  0.14267837,  0.20990627,  0.13482894,\n",
       "         -0.00364675,  0.21829607, -0.12877408,  0.20192634,  0.22213109,\n",
       "         -0.03388549, -0.06766225,  0.14448218,  0.22181372,  0.1328335 ,\n",
       "          0.15121876,  0.14171319, -0.17116542, -0.08145505,  0.03248198,\n",
       "          0.22020368,  0.22626232,  0.08720605,  0.17356892,  0.0067547 ,\n",
       "         -0.1671277 ,  0.13010935, -0.1650005 ,  0.1768881 ,  0.09859614,\n",
       "          0.04647444,  0.23818667,  0.20885377, -0.03810047, -0.14264843,\n",
       "         -0.17420268,  0.20959763, -0.20265236,  0.00364237, -0.0668346 ,\n",
       "          0.0280859 , -0.16306724, -0.09645668, -0.04871295,  0.09239723,\n",
       "         -0.02479221,  0.09142704, -0.07689014,  0.11410637,  0.23814206,\n",
       "          0.1916631 , -0.20160785, -0.04793169,  0.04738279,  0.03530563,\n",
       "         -0.06677298,  0.15083154, -0.02549812, -0.15253995,  0.13756575,\n",
       "          0.01007834, -0.12144512,  0.22143166,  0.07367073,  0.15285023,\n",
       "          0.09762926,  0.00537808, -0.10603993,  0.23100676,  0.03159393,\n",
       "         -0.2310344 , -0.09226125, -0.1475794 ,  0.15800439, -0.02385405,\n",
       "          0.04659493,  0.07212417, -0.03068945,  0.04060011, -0.07225513],\n",
       "        [ 0.02814864, -0.05682161, -0.07771021,  0.22870947,  0.01970457,\n",
       "          0.04543929, -0.00879635,  0.21686192,  0.12248759, -0.0990991 ,\n",
       "         -0.19200501,  0.09282269,  0.1047027 , -0.11364684, -0.13286287,\n",
       "          0.08594255, -0.095908  ,  0.1841368 ,  0.1935774 ,  0.06068535,\n",
       "         -0.0984267 , -0.09415907,  0.21231844,  0.11475636,  0.16140072,\n",
       "         -0.1575314 , -0.1661464 ,  0.11549853,  0.10145588, -0.22519302,\n",
       "          0.02286704, -0.02172434, -0.0312323 ,  0.17541344,  0.23776995,\n",
       "         -0.13109037,  0.09695257,  0.04687272, -0.2287764 , -0.16606939,\n",
       "         -0.09694739, -0.10621706,  0.1678418 ,  0.07627051,  0.07228558,\n",
       "         -0.02909261, -0.19905594, -0.02800074,  0.21364062,  0.03870471,\n",
       "          0.2283286 , -0.07035488, -0.03992276, -0.16309425,  0.18292238,\n",
       "         -0.08641703,  0.11879809,  0.05504762, -0.01648407, -0.14954469,\n",
       "         -0.08387959,  0.18721293, -0.17033355, -0.14513007,  0.09879254,\n",
       "         -0.01171513, -0.1868713 , -0.08643527, -0.20980464,  0.19434269,\n",
       "         -0.01087044, -0.04488444, -0.09295845, -0.21300708,  0.03579523,\n",
       "          0.21812977,  0.13407294,  0.13671957, -0.03176233,  0.05232076,\n",
       "          0.21156259, -0.22510377,  0.14811139, -0.19590971, -0.18776211,\n",
       "          0.22351183,  0.12380336,  0.22853275,  0.06891067,  0.13472094,\n",
       "          0.18049465, -0.13256523,  0.03333922, -0.20943967,  0.10626374,\n",
       "          0.11097489,  0.10811697,  0.13468806, -0.08514227,  0.22244845]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 0.18085872, -0.14481524,  0.07108589,  0.15595298, -0.23238587,\n",
       "         -0.23406386, -0.21760766, -0.0375983 , -0.20175496,  0.23114355,\n",
       "         -0.11212598,  0.06660162,  0.13310091, -0.13525748, -0.1979692 ,\n",
       "          0.23103131,  0.01167171,  0.07423262,  0.21973999, -0.0080478 ,\n",
       "         -0.18773144, -0.12562071, -0.09352565,  0.12074257,  0.16998641,\n",
       "         -0.0947555 ,  0.12088688, -0.10150141, -0.01482262,  0.19589381,\n",
       "          0.01732112, -0.03531927, -0.23605229,  0.13926505, -0.15277608,\n",
       "          0.06522809, -0.18537559, -0.08938642, -0.05762003, -0.05860168,\n",
       "         -0.10969631, -0.07219073,  0.2113512 ,  0.08014159, -0.22397691,\n",
       "         -0.21620905, -0.21404532,  0.03883915, -0.12380588,  0.06324239,\n",
       "          0.06444888, -0.00528495, -0.19566634, -0.17228618, -0.11626617,\n",
       "          0.12305368,  0.18170758, -0.05937006, -0.04210889, -0.0474399 ,\n",
       "         -0.17786357,  0.12798898, -0.23587292, -0.2201182 , -0.03020792,\n",
       "          0.04420562, -0.13440675,  0.00399527, -0.1397247 , -0.14438689,\n",
       "         -0.21783209,  0.03593604,  0.13873701,  0.09408496,  0.19262873,\n",
       "         -0.12020313, -0.20916198,  0.01528631,  0.12517776, -0.0376627 ,\n",
       "          0.1289467 , -0.01454096,  0.18745236,  0.21447943,  0.08839144,\n",
       "         -0.03344238,  0.16577788, -0.07989408,  0.13261865,  0.05598246,\n",
       "          0.11867811, -0.17373301,  0.03598203,  0.10934405, -0.01689072,\n",
       "         -0.08933342,  0.02479233, -0.16815625, -0.11337748,  0.19084378],\n",
       "        [-0.180903  ,  0.12576793, -0.10530528,  0.02774976,  0.14897983,\n",
       "         -0.23185128,  0.22544836, -0.081609  , -0.18538243, -0.13713881,\n",
       "         -0.21815348,  0.09669541,  0.16622214, -0.0094333 ,  0.04503565,\n",
       "         -0.17626674,  0.20535983, -0.23049524,  0.13260461, -0.215121  ,\n",
       "         -0.19903669, -0.20232984,  0.19900067,  0.1223871 , -0.15052804,\n",
       "         -0.03226615,  0.16029902, -0.03706986, -0.22276774,  0.20808785,\n",
       "          0.10978834, -0.16541448, -0.22867802,  0.16701771,  0.04542987,\n",
       "          0.13590957, -0.20192952,  0.01009345,  0.17791624,  0.20513742,\n",
       "         -0.19332838,  0.18335758,  0.1877098 ,  0.12937842,  0.04081784,\n",
       "         -0.13664092,  0.09527545,  0.09667946,  0.17293175,  0.17815368,\n",
       "          0.1837136 ,  0.00101921,  0.1207564 ,  0.03520869,  0.22315939,\n",
       "         -0.09779066, -0.17866376,  0.05223729, -0.03192551,  0.076474  ,\n",
       "          0.06161438, -0.19776921,  0.0898553 ,  0.11974604, -0.08681507,\n",
       "         -0.22072284, -0.14028704,  0.03573407, -0.12244438, -0.14165339,\n",
       "         -0.15382987, -0.15114465, -0.04506865,  0.14904453, -0.01675302,\n",
       "          0.17311971,  0.08910777,  0.18883403,  0.08090337,  0.13906522,\n",
       "         -0.00449601,  0.2070729 ,  0.2268254 ,  0.03384064,  0.21671687,\n",
       "         -0.15312749,  0.23456956,  0.10700135,  0.12596329, -0.02210152,\n",
       "         -0.20344166, -0.1081098 ,  0.11132677,  0.07183932, -0.00367656,\n",
       "         -0.01244527, -0.13098927, -0.06588338,  0.20184632, -0.16616127],\n",
       "        [-0.06706986,  0.19656534,  0.22473685, -0.04295997, -0.1852107 ,\n",
       "          0.09004413, -0.21060112, -0.04754791, -0.01248801, -0.15092039,\n",
       "         -0.1017964 , -0.09039599,  0.15734874, -0.07085089, -0.21152367,\n",
       "          0.08544125, -0.13887453, -0.03691678,  0.12649237, -0.19826083,\n",
       "         -0.20239772, -0.21358378,  0.23722418, -0.12688476, -0.12357689,\n",
       "          0.04445733,  0.14510684,  0.07425548,  0.01008165,  0.2225246 ,\n",
       "         -0.21700752,  0.209462  , -0.10091467, -0.18094781,  0.00484742,\n",
       "          0.18873556,  0.14917977, -0.15058738,  0.06198861,  0.22439267,\n",
       "          0.23183356, -0.14548549, -0.1017732 , -0.16706997,  0.02571242,\n",
       "         -0.03159192,  0.16403861, -0.22558445,  0.03953041, -0.10159972,\n",
       "         -0.0318822 , -0.00396295, -0.07311225, -0.01671347,  0.16376938,\n",
       "         -0.13916491, -0.18490973, -0.16101868, -0.18013884,  0.05458511,\n",
       "          0.12713604, -0.06072022,  0.22028188, -0.20857924, -0.06742294,\n",
       "         -0.1563586 , -0.02629597, -0.11754994,  0.18102174,  0.0537668 ,\n",
       "         -0.04144265,  0.04806529, -0.14519379,  0.08873208,  0.00254554,\n",
       "         -0.06101437, -0.01332688, -0.20873386, -0.0882387 , -0.02771829,\n",
       "         -0.184174  ,  0.02293999,  0.11730565,  0.2229215 ,  0.16491209,\n",
       "         -0.148821  , -0.17750064, -0.09383859, -0.13813749,  0.09636568,\n",
       "         -0.06951554, -0.04129548, -0.08776343, -0.21490312, -0.15075973,\n",
       "          0.14765926, -0.02140905,  0.18920381, -0.2046172 , -0.17634386],\n",
       "        [-0.14361542,  0.11124469, -0.18759796,  0.14287321, -0.02269602,\n",
       "          0.14735766, -0.06155095,  0.16386814, -0.00778604,  0.08349313,\n",
       "          0.09263997, -0.03845462,  0.18499573,  0.11237977, -0.14497118,\n",
       "         -0.16651605, -0.15391862,  0.10765944, -0.21192843,  0.02421065,\n",
       "         -0.2049172 ,  0.11332847,  0.14267837,  0.20990627,  0.13482894,\n",
       "         -0.00364675,  0.21829607, -0.12877408,  0.20192634,  0.22213109,\n",
       "         -0.03388549, -0.06766225,  0.14448218,  0.22181372,  0.1328335 ,\n",
       "          0.15121876,  0.14171319, -0.17116542, -0.08145505,  0.03248198,\n",
       "          0.22020368,  0.22626232,  0.08720605,  0.17356892,  0.0067547 ,\n",
       "         -0.1671277 ,  0.13010935, -0.1650005 ,  0.1768881 ,  0.09859614,\n",
       "          0.04647444,  0.23818667,  0.20885377, -0.03810047, -0.14264843,\n",
       "         -0.17420268,  0.20959763, -0.20265236,  0.00364237, -0.0668346 ,\n",
       "          0.0280859 , -0.16306724, -0.09645668, -0.04871295,  0.09239723,\n",
       "         -0.02479221,  0.09142704, -0.07689014,  0.11410637,  0.23814206,\n",
       "          0.1916631 , -0.20160785, -0.04793169,  0.04738279,  0.03530563,\n",
       "         -0.06677298,  0.15083154, -0.02549812, -0.15253995,  0.13756575,\n",
       "          0.01007834, -0.12144512,  0.22143166,  0.07367073,  0.15285023,\n",
       "          0.09762926,  0.00537808, -0.10603993,  0.23100676,  0.03159393,\n",
       "         -0.2310344 , -0.09226125, -0.1475794 ,  0.15800439, -0.02385405,\n",
       "          0.04659493,  0.07212417, -0.03068945,  0.04060011, -0.07225513],\n",
       "        [ 0.02814864, -0.05682161, -0.07771021,  0.22870947,  0.01970457,\n",
       "          0.04543929, -0.00879635,  0.21686192,  0.12248759, -0.0990991 ,\n",
       "         -0.19200501,  0.09282269,  0.1047027 , -0.11364684, -0.13286287,\n",
       "          0.08594255, -0.095908  ,  0.1841368 ,  0.1935774 ,  0.06068535,\n",
       "         -0.0984267 , -0.09415907,  0.21231844,  0.11475636,  0.16140072,\n",
       "         -0.1575314 , -0.1661464 ,  0.11549853,  0.10145588, -0.22519302,\n",
       "          0.02286704, -0.02172434, -0.0312323 ,  0.17541344,  0.23776995,\n",
       "         -0.13109037,  0.09695257,  0.04687272, -0.2287764 , -0.16606939,\n",
       "         -0.09694739, -0.10621706,  0.1678418 ,  0.07627051,  0.07228558,\n",
       "         -0.02909261, -0.19905594, -0.02800074,  0.21364062,  0.03870471,\n",
       "          0.2283286 , -0.07035488, -0.03992276, -0.16309425,  0.18292238,\n",
       "         -0.08641703,  0.11879809,  0.05504762, -0.01648407, -0.14954469,\n",
       "         -0.08387959,  0.18721293, -0.17033355, -0.14513007,  0.09879254,\n",
       "         -0.01171513, -0.1868713 , -0.08643527, -0.20980464,  0.19434269,\n",
       "         -0.01087044, -0.04488444, -0.09295845, -0.21300708,  0.03579523,\n",
       "          0.21812977,  0.13407294,  0.13671957, -0.03176233,  0.05232076,\n",
       "          0.21156259, -0.22510377,  0.14811139, -0.19590971, -0.18776211,\n",
       "          0.22351183,  0.12380336,  0.22853275,  0.06891067,  0.13472094,\n",
       "          0.18049465, -0.13256523,  0.03333922, -0.20943967,  0.10626374,\n",
       "          0.11097489,  0.10811697,  0.13468806, -0.08514227,  0.22244845]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Dense(*args, **kwargs)\n",
      " |  \n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "# print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_2 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\"\"\"\n",
    "        self.kernel = self.add_weight(name=\"kernel\",\n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer=\"uniform\",\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name=\"bias\",\n",
    "                                    shape=(self.units, ),\n",
    "                                    trainable=True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(0.001))\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 709us/step - loss: 5.0450 - val_loss: 1.0816\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.7073 - val_loss: 0.6565\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.6042 - val_loss: 0.6296\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.5820 - val_loss: 0.6115\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.5659 - val_loss: 0.5961\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 445us/step - loss: 0.5527 - val_loss: 0.5847\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.5419 - val_loss: 0.5730\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.5327 - val_loss: 0.5639\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 461us/step - loss: 0.5251 - val_loss: 0.5567\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.5186 - val_loss: 0.5497\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 442us/step - loss: 0.5135 - val_loss: 0.5451\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 440us/step - loss: 0.5088 - val_loss: 0.5385\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.5051 - val_loss: 0.5355\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.5014 - val_loss: 0.5317\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4977 - val_loss: 0.5300\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 440us/step - loss: 0.4944 - val_loss: 0.5236\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 437us/step - loss: 0.4915 - val_loss: 0.5213\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 439us/step - loss: 0.4887 - val_loss: 0.5168\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.4863 - val_loss: 0.5151\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 444us/step - loss: 0.4838 - val_loss: 0.5114\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4811 - val_loss: 0.5117\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 442us/step - loss: 0.4793 - val_loss: 0.5058\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4772 - val_loss: 0.5034\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.4748 - val_loss: 0.5003\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 445us/step - loss: 0.4735 - val_loss: 0.5034\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 440us/step - loss: 0.4716 - val_loss: 0.4982\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 434us/step - loss: 0.4702 - val_loss: 0.4971\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                   validation_data=(x_valid_scaled, y_valid),\n",
    "                   epochs=100,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 289us/step - loss: 0.4830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48302173614501953"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_gpu",
   "language": "python",
   "name": "tf2_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
