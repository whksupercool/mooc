{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "# print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state=7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state=11)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train mse: 1.7265313\t valid mse 1.4121678711479637n mse: 1.4460393 0 train mse: 1.450939 0 train mse: 1.8552402 0 train mse: 1.7458227\n",
      "Epoch 1 train mse: 1.2603776\t valid mse 1.4104680102010336in mse: 1.2043378 1 train mse: 1.2545521 1 train mse: 1.2402045 1 train mse: 1.2711815\n",
      "Epoch 2 train mse: 1.2539803\t valid mse 1.3957020846171566mse: 1.2406013 2 train mse: 1.2775275\n",
      "Epoch 3 train mse: 1.2386689\t valid mse 1.3911556052390834in mse: 1.258558 3 train mse: 1.2407291train mse: 1.2246895\n",
      "Epoch 4 train mse: 1.2761375\t valid mse 1.403000036167134in mse: 1.2653701 4 train mse: 1.2601323 4 train mse: 1.2860615 4 train mse: 1.2960681 1.2797136\n",
      "Epoch 5 train mse: 1.2764306\t valid mse 1.3903832045969167n mse: 1.298045 5 train mse: 1.29267575 train mse: 1.2746651 5 train mse: 1.2744298\n",
      "Epoch 6 train mse: 1.2736765\t valid mse 1.413327364560996in mse: 1.2618539 6 train mse: 1.2528597 1.2772194 6 train mse: 1.2797478 6 train mse: 1.2717242\n",
      "Epoch 7 train mse: 1.2752087\t valid mse 1.4025763634185353n mse: 1.252555 7 train mse: 1.2389861 7 train mse: 1.2624059 7 train mse: 1.2768474\n",
      "Epoch 8 train mse: 1.2494771\t valid mse 1.3957095383775648n mse: 1.196414 8 train mse: 1.2407628 8 train mse: 1.2582268 1.2631707 8 train mse: 1.2554556\n",
      "Epoch 9 train mse: 1.24076\t valid mse 1.3921407638120296in mse: 1.22398 9 train mse: 1.2383009 9 train mse: 1.23734061.2428132\n",
      "Epoch 10 train mse: 1.2543651\t valid mse 1.3884184972249725ain mse: 1.1912367 10 train mse: 1.2571406 10 train mse: 1.2493471\n",
      "Epoch 11 train mse: 1.2655983\t valid mse 1.393845505304544rain mse: 1.2282292 11 train mse: 1.2482964 11 train mse: 1.261261111 train mse: 1.2551478 11 train mse: 1.269586\n",
      "Epoch 12 train mse: 1.2614194\t valid mse 1.3876457522589551in mse: 1.268405 12 train mse: 1.2512814 12 train mse: 1.25691912 train mse: 1.2650896 12 train mse: 1.257793\n",
      "Epoch 13 train mse: 1.2402196\t valid mse 1.38767494257105806652 1.25274521.2353715 13 train mse: 1.2401434\n",
      "Epoch 14 train mse: 1.2385014\t valid mse 1.3933561216006274ain mse: 1.2507743 14 train mse: 1.2423193 14 train mse: 1.2434713 14 train mse: 1.2405303\n",
      "Epoch 15 train mse: 1.2274522\t valid mse 1.388839645992708train mse: 1.2752265 15 train mse: 1.236856 15 train mse: 1.2355565train mse: 1.2404826train mse: 1.2266556\n",
      "Epoch 16 train mse: 1.2607726\t valid mse 1.3868303312348182ain mse: 1.2621875 16 train mse: 1.2545483 16 train mse: 1.251351 16 train mse: 1.248362 16 train mse: 1.2604227\n",
      "Epoch 17 train mse: 1.2476424\t valid mse 1.3929851983419952446 17 train mse: 1.2354206 17 train mse: 1.2433128 17 train mse: 1.2466097 17 train mse: 1.2446148\n",
      "Epoch 18 train mse: 1.2681584\t valid mse 1.3884443605596848rain mse: 1.2347163 18 train mse: 1.2612255 train mse: 1.2498273 18 train mse: 1.2482743\n",
      "Epoch 19 train mse: 1.25886\t valid mse 1.3933591498333686train mse: 1.234314 19 train mse: 1.2303973 19 train mse: 1.2510575 19 train mse: 1.2650355 19 train mse: 1.2633358 19 train mse: 1.2586962\n",
      "Epoch 20 train mse: 1.2428421\t valid mse 1.3992350005829322ain mse: 1.2103783 20 train mse: 1.2430927 20 train mse: 1.2449675\n",
      "Epoch 21 train mse: 1.2517693\t valid mse 1.3868772064525448in mse: 1.257185 21 train mse: 1.268935 21 train mse: 1.2697374 21 train mse: 1.2463831\n",
      "Epoch 22 train mse: 1.2471524\t valid mse 1.3871836196178664ain mse: 1.2837998 22 train mse: 1.2909704 22 train mse: 1.2531606 22 train mse: 1.24928 22 train mse: 1.2400254\n",
      "Epoch 23 train mse: 1.2550403\t valid mse 1.3866925046311098mse: 1.1828616 23 train mse: 1.1945121 23 train mse: 1.2335067 23 train mse: 1.2530291\n",
      "Epoch 24 train mse: 1.2726341\t valid mse 1.3921683266993958rain mse: 1.2688055 24 train mse: 1.2692442 24 train mse: 1.2837893 24 train mse: 1.2725475 24 train mse: 1.2790667 24 train mse: 1.2732745\n",
      "Epoch 25 train mse: 1.2708577\t valid mse 1.3863378346087887921 25 train mse: 1.2630228 25 train mse: 1.270582 25 train mse: 1.2713878\n",
      "Epoch 26 train mse: 1.232128\t valid mse 1.3875096411608916rain mse: 1.2366648 26 train mse: 1.2468137 26 train mse: 1.2431254 26 train mse: 1.2438616 26 train mse: 1.2292635\n",
      "Epoch 27 train mse: 1.2700286\t valid mse 1.3861992535128684rain mse: 1.2756733 27 train mse: 1.2894003 27 train mse: 1.2863855 27 train mse: 1.2706034 27 train mse: 1.2657307\n",
      "Epoch 28 train mse: 1.2634577\t valid mse 1.3874960710148891ain mse: 1.2499845 28 train mse: 1.2825208 28 train mse: 1.27504271.2732042\n",
      "Epoch 29 train mse: 1.2284735 29 train mse: 1.2370785 29 train mse: 1.2636731 29 train mse: 1.226927 29 train mse: 1.2258751 29 train mse: 1.2216344 29 train mse: 1.2282345\t valid mse 1.3946684555681221\n",
      "Epoch 30 train mse: 1.2878126\t valid mse 1.3960093373359679mse: 1.2617437 30 train mse: 1.2752491 30 train mse: 1.2860413\n",
      "Epoch 31 train mse: 1.2863095\t valid mse 1.393539812673093rain mse: 1.319453 31 train mse: 1.308774 31 train mse: 1.3000422 31 train mse: 1.2963157 31 train mse: 1.2933425\n",
      "Epoch 32 train mse: 1.2566254\t valid mse 1.3858558988052916in mse: 1.2064307 32 train mse: 1.2587618 32 train mse: 1.2553467 32 train mse: 1.2519912\n",
      "Epoch 33 train mse: 1.2573482\t valid mse 1.3997568442274027rain mse: 1.2907541 33 train mse: 1.2659968 33 train mse: 1.2620629 33 train mse: 1.2635834\n",
      "Epoch 34 train mse: 1.2500077\t valid mse 1.391071526046094rain mse: 1.2502289 34 train mse: 1.23491921.2476329 34 train mse: 1.2509025\n",
      "Epoch 35 train mse: 1.242001\t valid mse 1.3846523608342336rain mse: 1.250798 35 train mse: 1.2364582 35 train mse: 1.2390876\n",
      "Epoch 36 train mse: 1.2730464\t valid mse 1.389335114060949n mse: 1.2665175 36 train mse: 1.2691813 36 train mse: 1.262474 1.267002936 train mse: 1.2659051\n",
      "Epoch 37 train mse: 1.264927\t valid mse 1.3850421100863564rain mse: 1.2837992 37 train mse: 1.2695454 37 train mse: 1.2845504 37 train mse: 1.2747395 37 train mse: 1.2594899\n",
      "Epoch 38 train mse: 1.2383729\t valid mse 1.386878771159685rain mse: 1.2291844 38 train mse: 1.2339382 38 train mse: 1.2435013 38 train mse: 1.228508 38 train mse: 1.2308254\n",
      "Epoch 39 train mse: 1.2800269\t valid mse 1.3922700471111151ain mse: 1.2800838 39 train mse: 1.2783448 train mse: 1.280626\n",
      "Epoch 40 train mse: 1.2487837\t valid mse 1.3894925451653322ain mse: 1.2220404 40 train mse: 1.215475 40 train mse: 1.2274176 40 train mse: 1.2357583 40 train mse: 1.2497493\n",
      "Epoch 41 train mse: 1.2717506\t valid mse 1.3883243321903107ain mse: 1.2808927 41 train mse: 1.2717719 41 train mse: 1.3001792 41 train mse: 1.2873468 41 train mse: 1.2779315\n",
      "Epoch 42 train mse: 1.2787889\t valid mse 1.3903609803648032rain mse: 1.3060648 42 train mse: 1.3064705 42 train mse: 1.2905504 42 train mse: 1.2871146 42 train mse: 1.2802348\n",
      "Epoch 43 train mse: 1.2607334\t valid mse 1.393672251999502rain mse: 1.2866776 43 train mse: 1.278001 43 train mse: 1.262154 43 train mse: 1.2598989\n",
      "Epoch 44 train mse: 1.2454518\t valid mse 1.4022378817802381in mse: 1.3387866 44 train mse: 1.266283 train mse: 1.2714045 44 train mse: 1.2460823 44 train mse: 1.2421331\n",
      "Epoch 45 train mse: 1.2361206 45 train mse: 1.2632471 45 train mse: 1.2039745 45 train mse: 1.2314228 45 train mse: 1.2287302 1.2348608\t valid mse 1.3901122338983274\n",
      "Epoch 46 train mse: 1.2567788\t valid mse 1.3973936632920425ain mse: 1.2613157 46 train mse: 1.2660546 train mse: 1.2600206 46 train mse: 1.2571582\n",
      "Epoch 47 train mse: 1.2623254\t valid mse 1.387633763246053rain mse: 1.2961974 47 train mse: 1.2822266 47 train mse: 1.2728925train mse: 1.2602469\n",
      "Epoch 48 train mse: 1.2643247\t valid mse 1.3929269498244847in mse: 1.2760134 48 train mse: 1.2649813 48 train mse: 1.2558974 48 train mse: 1.258648\n",
      "Epoch 49 train mse: 1.2484506\t valid mse 1.3858826325962965ain mse: 1.2367016 49 train mse: 1.245595349 train mse: 1.261637449 train mse: 1.2400696 49 train mse: 1.2384036\n",
      "Epoch 50 train mse: 1.2614259\t valid mse 1.388276168580742rain mse: 1.2954692 50 train mse: 1.250761 50 train mse: 1.2540276 50 train mse: 1.2583141\n",
      "Epoch 51 train mse: 1.2657052\t valid mse 1.3887364675436884ain mse: 1.2933755 51 train mse: 1.26674321.2685177 51 train mse: 1.2657392\n",
      "Epoch 52 train mse: 1.25033\t valid mse 1.3984752354090997 train mse: 1.2076547 52 train mse: 1.2160181 52 train mse: 1.2469821\n",
      "Epoch 53 train mse: 1.2469988\t valid mse 1.3907274654549306ain mse: 1.2428821 53 train mse: 1.251257 53 train mse: 1.2378044 53 train mse: 1.2523323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 train mse: 1.264908\t valid mse 1.3860811550516994rain mse: 1.2643845 54 train mse: 1.2616243\n",
      "Epoch 55 train mse: 1.2609572\t valid mse 1.3867827461510769ain mse: 1.2668539 55 train mse: 1.2816666 55 train mse: 1.2633994 55 train mse: 1.2502728 55 train mse: 1.2562932\n",
      "Epoch 56 train mse: 1.2421294\t valid mse 1.3866101008071796ain mse: 1.25903556 train mse: 1.2409753 56 train mse: 1.2418559 56 train mse: 1.2409053\n",
      "Epoch 57 train mse: 1.2690573\t valid mse 1.385553760713143rain mse: 1.2685667 57 train mse: 1.2644758 57 train mse: 1.2727977 57 train mse: 1.2702231\n",
      "Epoch 58 train mse: 1.2776836\t valid mse 1.3882769209931733ain mse: 1.3112051 58 train mse: 1.27912081.2788546 58 train mse: 1.2891331 58 train mse: 1.2802982\n",
      "Epoch 59 train mse: 1.2296664\t valid mse 1.400388787451445train mse: 1.2177173 59 train mse: 1.2159101 59 train mse: 1.2306213 59 train mse: 1.2379559\n",
      "Epoch 60 train mse: 1.2803367\t valid mse 1.384719935495547train mse: 1.2847116 60 train mse: 1.2667186 60 train mse: 1.2609552 1.261427\n",
      "Epoch 61 train mse: 1.2636015\t valid mse 1.3850382182945442ain mse: 1.2670804 61 train mse: 1.256711 61 train mse: 1.2604004\n",
      "Epoch 62 train mse: 1.218384\t valid mse 1.388516792368218538477 62 train mse: 1.197644 62 train mse: 1.2142332 1.2201803 62 train mse: 1.222243\n",
      "Epoch 63 train mse: 1.2480122\t valid mse 1.3858735432824865ain mse: 1.2267191 63 train mse: 1.2303667 63 train mse: 1.2554791 63 train mse: 1.2526822 63 train mse: 1.2492733\n",
      "Epoch 64 train mse: 1.2495564\t valid mse 1.3873247783736737rain mse: 1.275638 64 train mse: 1.247631 64 train mse: 1.2463834 64 train mse: 1.2502126 1.2520608\n",
      "Epoch 65 train mse: 1.2570734\t valid mse 1.3866082771582267ain mse: 1.2722967 1.2814503 65 train mse: 1.2773825 65 train mse: 1.2877685 65 train mse: 1.270041\n",
      "Epoch 66 train mse: 1.2501916\t valid mse 1.38632410150025161038train mse: 1.2278531 66 train mse: 1.238051 66 train mse: 1.2386853 66 train mse: 1.2502825\n",
      "Epoch 67 train mse: 1.2124243\t valid mse 1.38663717532598rain mse: 1.1519719 67 train mse: 1.1704121 67 train mse: 1.1717701 67 train mse: 1.1928368 67 train mse: 1.2058437\n",
      "Epoch 68 train mse: 1.2552392\t valid mse 1.38569772376738train mse: 1.2889843 68 train mse: 1.254098 68 train mse: 1.2521355 68 train mse: 1.2550193\n",
      "Epoch 69 train mse: 1.275737\t valid mse 1.3846161370708403rain mse: 1.2085088 69 train mse: 1.2436967 69 train mse: 1.2359171 69 train mse: 1.2626104 69 train mse: 1.2713042\n",
      "Epoch 70 train mse: 1.2728376\t valid mse 1.386373483021459rain mse: 1.3078836 70 train mse: 1.2776415 70 train mse: 1.2686876 70 train mse: 1.2677983\n",
      "Epoch 71 train mse: 1.2421374 71 train mse: 1.4070611 71 train mse: 1.2926306 71 train mse: 1.2957383 71 train mse: 1.2589781 71 train mse: 1.2469085 71 train mse: 1.2432141\t valid mse 1.3884301894952844\n",
      "Epoch 72 train mse: 1.2403774\t valid mse 1.3873356436803028ain mse: 1.2684938 72 train mse: 1.2630955 72 train mse: 1.2464806train mse: 1.2532079 72 train mse: 1.2389078\n",
      "Epoch 73 train mse: 1.2889125\t valid mse 1.3859421163262557ain mse: 1.298934 73 train mse: 1.2880986 73 train mse: 1.2905188 73 train mse: 1.2903091\n",
      "Epoch 74 train mse: 1.2447584\t valid mse 1.3840687923223702 mse: 1.2328955 74 train mse: 1.244857 74 train mse: 1.252637474 train mse: 1.2329656\n",
      "Epoch 75 train mse: 1.2464389\t valid mse 1.3842638286814835rain mse: 1.236664575 train mse: 1.2449241 75 train mse: 1.2419201\n",
      "Epoch 76 train mse: 1.2689368\t 1.2439165 76 train mse: 1.2851862 76 train mse: 1.2808577train mse: 1.2842708 valid mse 1.3962235664568832\n",
      "Epoch 77 train mse: 1.2772489\t valid mse 1.3895510874592425ain mse: 1.2942016 77 train mse: 1.2975981 77 train mse: 1.2922534 77 train mse: 1.2729919\n",
      "Epoch 78 train mse: 1.2381201\t valid mse 1.386615957175359train mse: 1.2888983 train mse: 1.2479708 78 train mse: 1.240984 78 train mse: 1.2271799 78 train mse: 1.2389755\n",
      "Epoch 79 train mse: 1.2609525\t valid mse 1.3881871610117817ain mse: 1.2582567 79 train mse: 1.2620999 79 train mse: 1.2687911 79 train mse: 1.271071\n",
      "Epoch 80 train mse: 1.2519394\t valid mse 1.3877233360138552ain mse: 1.2606655 80 train mse: 1.2561159 80 train mse: 1.2508289 80 train mse: 1.2511388\n",
      "Epoch 81 train mse: 1.2969398\t valid mse 1.3861638043320692in mse: 1.3036112 81 train mse: 1.320807 train mse: 1.2946943 81 train mse: 1.284926 81 train mse: 1.2908757\n",
      "Epoch 82 train mse: 1.2928829\t valid mse 1.3865618426391528ain mse: 1.2778115 82 train mse: 1.2735248 82 train mse: 1.2721003 82 train mse: 1.2814319\n",
      "Epoch 83 train mse: 1.2486287\t valid mse 1.3845146368928007ain mse: 1.2588714 83 train mse: 1.2297533 83 train mse: 1.2486534 83 train mse: 1.2455304\n",
      "Epoch 84 train mse: 1.2430142\t valid mse 1.3848994560376915ain mse: 1.2525553 84 train mse: 1.2545112 1.2382728 84 train mse: 1.2410295 84 train mse: 1.2413982\n",
      "Epoch 85 train mse: 1.2786853\t valid mse 1.3883054188263884ain mse: 1.2411492 85 train mse: 1.2387676 85 train mse: 1.2665269 85 train mse: 1.276927 85 train mse: 1.2719465\n",
      "Epoch 86 train mse: 1.2392894\t valid mse 1.3841705055774225mse: 1.235502 86 train mse: 1.2435014 86 train mse: 1.245979\n",
      "Epoch 87 train mse: 1.2591717\t valid mse 1.38404823294161424891 87 train mse: 1.2428391 train mse: 1.2406932 87 train mse: 1.2593942\n",
      "Epoch 88 train mse: 1.2599529\t valid mse 1.3858540098753078ain mse: 1.2804488 88 train mse: 1.2917144 88 train mse: 1.2738038 88 train mse: 1.2589667\n",
      "Epoch 89 train mse: 1.275439\t valid mse 1.385700877319227train mse: 1.2725167 89 train mse: 1.2670349 89 train mse: 1.2841039 89 train mse: 1.2694843\n",
      "Epoch 90 train mse: 1.2613043\t valid mse 1.3835489015159141ain mse: 1.1998109 90 train mse: 1.228746 90 train mse: 1.2269423 90 train mse: 1.2574263 90 train mse: 1.2607563\n",
      "Epoch 91 train mse: 1.2878598\t valid mse 1.3839106756693258ain mse: 1.3154428 91 train mse: 1.302866 91 train mse: 1.2935941 91 train mse: 1.3014964 1.2909786\n",
      "Epoch 92 train mse: 1.2397002 92 train mse: 1.2301941 92 train mse: 1.2650583 92 train mse: 1.2254736 92 train mse: 1.2304925 92 train mse: 1.2298015 92 train mse: 1.2403723\t valid mse 1.389670686685651\n",
      "Epoch 93 train mse: 1.2419916\t valid mse 1.3896496172716961rain mse: 1.2535307 93 train mse: 1.26088521.2397889\n",
      "Epoch 94 train mse: 1.3045574\t valid mse 1.384673963031167ain mse: 1.3439361 94 train mse: 1.3286856 94 train mse: 1.3127321 94 train mse: 1.3124714 94 train mse: 1.3127043\n",
      "Epoch 95 train mse: 1.2566085\t valid mse 1.3854465581077842ain mse: 1.245761 95 train mse: 1.2505817 95 train mse: 1.2582725\n",
      "Epoch 96 train mse: 1.2382964\t valid mse 1.3846520208018278ain mse: 1.1963521 96 train mse: 1.20061891.2197951 96 train mse: 1.2319553\n",
      "Epoch 97 train mse: 1.220863\t valid mse 1.3868357854742075rain mse: 1.2867335 97 train mse: 1.2417963 97 train mse: 1.227425 97 train mse: 1.2303418\n",
      "Epoch 98 train mse: 1.3036618\t valid mse 1.3847555064578305rain mse: 1.2959826 98 train mse: 1.3166106 98 train mse: 1.299479 98 train mse: 1.3077388 98 train mse: 1.3029317\n",
      "Epoch 99 train mse: 1.2595074\t valid mse 1.387473482445639rain mse: 1.2334076 99 train mse: 1.2507463 99 train mse: 1.2499679 99 train mse: 1.2607011\n"
     ]
    }
   ],
   "source": [
    "epochs =100\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train_scaled) // batch_size\n",
    "optimizer = keras.optimizers.SGD()\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "\n",
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "                 \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    for step in range(steps_per_epoch):\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred  = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print(\"\\rEpoch\", epoch, \"train mse:\", metric.result().numpy(), end=\"\")\n",
    "    y_valid_pred = model(x_valid_scaled)\n",
    "    valid_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_valid_pred, y_valid))\n",
    "    print(\"\\t\", \"valid mse\", valid_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3a082fe5c6a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplot_learning_curves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_gpu",
   "language": "python",
   "name": "tf2_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
